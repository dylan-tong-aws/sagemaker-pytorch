{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MNIST Training using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Host](#Host)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). This tutorial will show how to train and test an MNIST model on SageMaker using PyTorch.\n",
    "\n",
    "For more information about the PyTorch in SageMaker, please visit [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers) and [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) github repositories.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "Let's start by creating a SageMaker session and specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `sagemaker.get_execution_role()` with a the appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-mnist'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Getting the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "datasets.MNIST('data', download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the data to S3\n",
    "We are going to use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value inputs identifies the location -- we will use later when we start the training job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-west-2-407247006381/sagemaker/DEMO-pytorch-mnist\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix=prefix)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Training script\n",
    "The `mnist.py` script provides all the code we need for training and hosting a SageMaker model (`model_fn` function to load a model).\n",
    "The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to.\n",
    "  These artifacts are uploaded to S3 for model hosting.\n",
    "* `SM_NUM_GPUS`: The number of gpus available in the current container.\n",
    "* `SM_CURRENT_HOST`: The name of the current container on the container network.\n",
    "* `SM_HOSTS`: JSON encoded list containing all the hosts .\n",
    "\n",
    "Supposing one input channel, 'training', was used in the call to the PyTorch estimator's `fit()` method, the following will be set, following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "* `SM_CHANNEL_TRAINING`: A string representing the path to the directory containing data in the 'training' channel.\n",
    "\n",
    "For more information about training environment variables, please visit [SageMaker Containers](https://github.com/aws/sagemaker-containers).\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance.\n",
    "\n",
    "Because the SageMaker imports the training script, you should put your training code in a main guard (``if __name__=='__main__':``) if you are using the same script to host your model as we do in this example, so that SageMaker does not inadvertently run your training code at the wrong point in execution.\n",
    "\n",
    "For example, the script run by this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.distributed\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdist\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn.functional\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mF\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.utils.data\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.utils.data.distributed\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datasets, transforms\n",
      "\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
      "\n",
      "\n",
      "\u001b[37m# Based on https://github.com/pytorch/examples/blob/master/mnist/main.py\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mNet\u001b[39;49;00m(nn.Module):\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[36msuper\u001b[39;49;00m(Net, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\n",
      "        \u001b[36mself\u001b[39;49;00m.conv1 = nn.Conv2d(\u001b[34m1\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m, kernel_size=\u001b[34m5\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.conv2 = nn.Conv2d(\u001b[34m10\u001b[39;49;00m, \u001b[34m20\u001b[39;49;00m, kernel_size=\u001b[34m5\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.conv2_drop = nn.Dropout2d()\n",
      "        \u001b[36mself\u001b[39;49;00m.fc1 = nn.Linear(\u001b[34m320\u001b[39;49;00m, \u001b[34m50\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.fc2 = nn.Linear(\u001b[34m50\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\n",
      "        x = F.relu(F.max_pool2d(\u001b[36mself\u001b[39;49;00m.conv1(x), \u001b[34m2\u001b[39;49;00m))\n",
      "        x = F.relu(F.max_pool2d(\u001b[36mself\u001b[39;49;00m.conv2_drop(\u001b[36mself\u001b[39;49;00m.conv2(x)), \u001b[34m2\u001b[39;49;00m))\n",
      "        x = x.view(-\u001b[34m1\u001b[39;49;00m, \u001b[34m320\u001b[39;49;00m)\n",
      "        x = F.relu(\u001b[36mself\u001b[39;49;00m.fc1(x))\n",
      "        x = F.dropout(x, training=\u001b[36mself\u001b[39;49;00m.training)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.fc2(x)\n",
      "        \u001b[34mreturn\u001b[39;49;00m F.log_softmax(x, dim=\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_train_data_loader\u001b[39;49;00m(batch_size, training_dir, is_distributed, **kwargs):\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet train data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    dataset = datasets.MNIST(training_dir, train=\u001b[36mTrue\u001b[39;49;00m, transform=transforms.Compose([\n",
      "        transforms.ToTensor(),\n",
      "        transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))\n",
      "    ]))\n",
      "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset) \u001b[34mif\u001b[39;49;00m is_distributed \u001b[34melse\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=train_sampler \u001b[35mis\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m,\n",
      "                                       sampler=train_sampler, **kwargs)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_get_test_data_loader\u001b[39;49;00m(test_batch_size, training_dir, **kwargs):\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet test data loader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m torch.utils.data.DataLoader(\n",
      "        datasets.MNIST(training_dir, train=\u001b[36mFalse\u001b[39;49;00m, transform=transforms.Compose([\n",
      "            transforms.ToTensor(),\n",
      "            transforms.Normalize((\u001b[34m0.1307\u001b[39;49;00m,), (\u001b[34m0.3081\u001b[39;49;00m,))\n",
      "        ])),\n",
      "        batch_size=test_batch_size, shuffle=\u001b[36mTrue\u001b[39;49;00m, **kwargs)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_average_gradients\u001b[39;49;00m(model):\n",
      "    \u001b[37m# Gradient averaging.\u001b[39;49;00m\n",
      "    size = \u001b[36mfloat\u001b[39;49;00m(dist.get_world_size())\n",
      "    \u001b[34mfor\u001b[39;49;00m param \u001b[35min\u001b[39;49;00m model.parameters():\n",
      "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n",
      "        param.grad.data /= size\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(args):\n",
      "    is_distributed = \u001b[36mlen\u001b[39;49;00m(args.hosts) > \u001b[34m1\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m args.backend \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mDistributed training - {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(is_distributed))\n",
      "    use_cuda = args.num_gpus > \u001b[34m0\u001b[39;49;00m\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mNumber of gpus available - {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.num_gpus))\n",
      "    kwargs = {\u001b[33m'\u001b[39;49;00m\u001b[33mnum_workers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpin_memory\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[36mTrue\u001b[39;49;00m} \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m {}\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m use_cuda \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m is_distributed:\n",
      "        \u001b[37m# Initialize the distributed environment.\u001b[39;49;00m\n",
      "        world_size = \u001b[36mlen\u001b[39;49;00m(args.hosts)\n",
      "        os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mWORLD_SIZE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(world_size)\n",
      "        host_rank = args.hosts.index(args.current_host)\n",
      "        os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mRANK\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(host_rank)\n",
      "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mInitialized the distributed environment: \u001b[39;49;00m\u001b[33m\\'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\'\u001b[39;49;00m\u001b[33m backend on {} nodes. \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "            args.backend, dist.get_world_size()) + \u001b[33m'\u001b[39;49;00m\u001b[33mCurrent host rank is {}. Number of gpus: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "            dist.get_rank(), args.num_gpus))\n",
      "\n",
      "    \u001b[37m# set the seed for generating random numbers\u001b[39;49;00m\n",
      "    torch.manual_seed(args.seed)\n",
      "    \u001b[34mif\u001b[39;49;00m use_cuda:\n",
      "        torch.cuda.manual_seed(args.seed)\n",
      "\n",
      "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir, is_distributed, **kwargs)\n",
      "    test_loader = _get_test_data_loader(args.test_batch_size, args.data_dir, **kwargs)\n",
      "\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses {}/{} ({:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of train data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "        \u001b[36mlen\u001b[39;49;00m(train_loader.sampler), \u001b[36mlen\u001b[39;49;00m(train_loader.dataset),\n",
      "        \u001b[34m100.\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(train_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(train_loader.dataset)\n",
      "    ))\n",
      "\n",
      "    logger.debug(\u001b[33m\"\u001b[39;49;00m\u001b[33mProcesses {}/{} ({:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m) of test data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "        \u001b[36mlen\u001b[39;49;00m(test_loader.sampler), \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\n",
      "        \u001b[34m100.\u001b[39;49;00m * \u001b[36mlen\u001b[39;49;00m(test_loader.sampler) / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\n",
      "    ))\n",
      "\n",
      "    model = Net().to(device)\n",
      "    \u001b[34mif\u001b[39;49;00m is_distributed \u001b[35mand\u001b[39;49;00m use_cuda:\n",
      "        \u001b[37m# multi-machine multi-gpu case\u001b[39;49;00m\n",
      "        model = torch.nn.parallel.DistributedDataParallel(model)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[37m# single-machine multi-gpu case or single-machine or multi-machine cpu case\u001b[39;49;00m\n",
      "        model = torch.nn.DataParallel(model)\n",
      "\n",
      "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, args.epochs + \u001b[34m1\u001b[39;49;00m):\n",
      "        model.train()\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_idx, (data, target) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader, \u001b[34m1\u001b[39;49;00m):\n",
      "            data, target = data.to(device), target.to(device)\n",
      "            optimizer.zero_grad()\n",
      "            output = model(data)\n",
      "            loss = F.nll_loss(output, target)\n",
      "            loss.backward()\n",
      "            \u001b[34mif\u001b[39;49;00m is_distributed \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m use_cuda:\n",
      "                \u001b[37m# average gradients manually for multi-machine cpu case only\u001b[39;49;00m\n",
      "                _average_gradients(model)\n",
      "            optimizer.step()\n",
      "            \u001b[34mif\u001b[39;49;00m batch_idx % args.log_interval == \u001b[34m0\u001b[39;49;00m:\n",
      "                logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTrain Epoch: {} [{}/{} ({:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)] Loss: {:.6f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "                    epoch, batch_idx * \u001b[36mlen\u001b[39;49;00m(data), \u001b[36mlen\u001b[39;49;00m(train_loader.sampler),\n",
      "                    \u001b[34m100.\u001b[39;49;00m * batch_idx / \u001b[36mlen\u001b[39;49;00m(train_loader), loss.item()))\n",
      "        test(model, test_loader, device)\n",
      "    save_model(model, args.model_dir)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(model, test_loader, device):\n",
      "    model.eval()\n",
      "    test_loss = \u001b[34m0\u001b[39;49;00m\n",
      "    correct = \u001b[34m0\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        \u001b[34mfor\u001b[39;49;00m data, target \u001b[35min\u001b[39;49;00m test_loader:\n",
      "            data, target = data.to(device), target.to(device)\n",
      "            output = model(data)\n",
      "            test_loss += F.nll_loss(output, target, size_average=\u001b[36mFalse\u001b[39;49;00m).item()  \u001b[37m# sum up batch loss\u001b[39;49;00m\n",
      "            pred = output.max(\u001b[34m1\u001b[39;49;00m, keepdim=\u001b[36mTrue\u001b[39;49;00m)[\u001b[34m1\u001b[39;49;00m]  \u001b[37m# get the index of the max log-probability\u001b[39;49;00m\n",
      "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
      "\n",
      "    test_loss /= \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "        test_loss, correct, \u001b[36mlen\u001b[39;49;00m(test_loader.dataset),\n",
      "        \u001b[34m100.\u001b[39;49;00m * correct / \u001b[36mlen\u001b[39;49;00m(test_loader.dataset)))\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model = torch.nn.DataParallel(Net())\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        model.load_state_dict(torch.load(f))\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(model, model_dir):\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[37m# recommended way from http://pytorch.org/docs/master/notes/serialization.html\u001b[39;49;00m\n",
      "    torch.save(model.cpu().state_dict(), path)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# Data and model checkpoints directories\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1000\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for testing (default: 1000)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.5\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mSGD momentum (default: 0.5)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m100\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--backend\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[36mNone\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mbackend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Container environment\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "    train(parser.parse_args())\n"
     ]
    }
   ],
   "source": [
    "!pygmentize mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training in SageMaker\n",
    "\n",
    "The `PyTorch` class allows us to run our training function as a training job on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, the training instance type, and hyperparameters. In this case we are going to run our training job on 2 ```ml.c4.xlarge``` instances. But this example can be ran on one or multiple, cpu or gpu instances ([full list of available instances](https://aws.amazon.com/sagemaker/pricing/instance-types/)). The hyperparameters parameter is a dict of values that will be passed to your training script -- you can see how to access these values in the `mnist.py` script above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SageMaker Local\n",
    "\n",
    "Before we launch a SageMaker training job, we can use SageMaker Local to deploy and run our PyTorch training container on our SageMaker Notebook instance. This gives us a means to test our script locally in the training container before we launch a remote training job. We create a local training job by specifying `local` as our instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "local_estimator = PyTorch(entry_point='mnist.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.2.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='local',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 1\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker Local allows you to serve the data set from the local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tmpca5bv1_algo-1-0hxaz_1 ... \n",
      "\u001b[1BAttaching to tmpca5bv1_algo-1-0hxaz_12mdone\u001b[0m\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m 2020-01-22 06:55:22,220 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m 2020-01-22 06:55:22,224 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m 2020-01-22 06:55:22,242 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m 2020-01-22 06:55:22,246 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m 2020-01-22 06:55:22,455 sagemaker-containers INFO     Module mnist does not provide a setup.py. \n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Generating setup.py\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m 2020-01-22 06:55:22,456 sagemaker-containers INFO     Generating setup.cfg\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m 2020-01-22 06:55:22,456 sagemaker-containers INFO     Generating MANIFEST.in\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m 2020-01-22 06:55:22,457 sagemaker-containers INFO     Installing module with the following command:\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m /opt/conda/bin/python -m pip install . \n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Processing /opt/ml/code\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Building wheels for collected packages: mnist\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m   Building wheel for mnist (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m \u001b[?25h  Created wheel for mnist: filename=mnist-1.0.0-py2.py3-none-any.whl size=9443 sha256=f46a6c3163efe758d55f6fd22c277619645a4d8a00030ca56e6479f5445e9dcb\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m   Stored in directory: /tmp/pip-ephem-wheel-cache-r8hl6koy/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Successfully built mnist\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Installing collected packages: mnist\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Successfully installed mnist-1.0.0\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m \u001b[33mWARNING: You are using pip version 19.3; however, version 20.0.1 is available.\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m 2020-01-22 06:55:24,466 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m 2020-01-22 06:55:24,481 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Training Env:\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m {\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m         \"training\": \"/opt/ml/input/data/training\"\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"current_host\": \"algo-1-0hxaz\",\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"hosts\": [\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m         \"algo-1-0hxaz\"\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     ],\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m         \"epochs\": 1\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m         \"training\": {\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m         }\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"job_name\": \"pytorch-training-2020-01-22-06-53-52-353\",\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"master_hostname\": \"algo-1-0hxaz\",\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"module_dir\": \"s3://sagemaker-us-west-2-407247006381/pytorch-training-2020-01-22-06-53-52-353/source/sourcedir.tar.gz\",\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"module_name\": \"mnist\",\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"num_cpus\": 2,\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m         \"hosts\": [\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m             \"algo-1-0hxaz\"\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m         ],\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m         \"current_host\": \"algo-1-0hxaz\"\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     },\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m     \"user_entry_point\": \"mnist.py\"\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Environment variables:\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_HOSTS=[\"algo-1-0hxaz\"]\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_HPS={\"epochs\":1}\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_USER_ENTRY_POINT=mnist.py\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-0hxaz\",\"hosts\":[\"algo-1-0hxaz\"]}\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_INPUT_DATA_CONFIG={\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_CHANNELS=[\"training\"]\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_CURRENT_HOST=algo-1-0hxaz\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_MODULE_NAME=mnist\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_NUM_CPUS=2\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_MODULE_DIR=s3://sagemaker-us-west-2-407247006381/pytorch-training-2020-01-22-06-53-52-353/source/sourcedir.tar.gz\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-0hxaz\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-0hxaz\"],\"hyperparameters\":{\"epochs\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-01-22-06-53-52-353\",\"log_level\":20,\"master_hostname\":\"algo-1-0hxaz\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-407247006381/pytorch-training-2020-01-22-06-53-52-353/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-0hxaz\",\"hosts\":[\"algo-1-0hxaz\"]},\"user_entry_point\":\"mnist.py\"}\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"1\"]\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m PYTHONPATH=/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m /opt/conda/bin/python -m mnist --epochs 1\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Distributed training - False\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Number of gpus available - 0\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Get train data loader\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Get test data loader\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Processes 60000/60000 (100%) of train data\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Processes 10000/10000 (100%) of test data\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Train Epoch: 1 [6400/60000 (11%)] Loss: 1.935915\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Train Epoch: 1 [12800/60000 (21%)] Loss: 1.211672\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Train Epoch: 1 [19200/60000 (32%)] Loss: 0.880198\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Train Epoch: 1 [25600/60000 (43%)] Loss: 0.596671\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Train Epoch: 1 [32000/60000 (53%)] Loss: 0.537026\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Train Epoch: 1 [38400/60000 (64%)] Loss: 0.698346\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Train Epoch: 1 [44800/60000 (75%)] Loss: 0.479144\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Train Epoch: 1 [51200/60000 (85%)] Loss: 0.595473\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Train Epoch: 1 [57600/60000 (96%)] Loss: 0.438032\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Test set: Average loss: 0.2064, Accuracy: 9392/10000 (94%)\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m \n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m Saving the model.\n",
      "\u001b[36malgo-1-0hxaz_1  |\u001b[0m 2020-01-22 06:55:44,531 sagemaker-containers INFO     Reporting training SUCCESS\n",
      "\u001b[36mtmpca5bv1_algo-1-0hxaz_1 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "local_estimator.fit({\"training\": \"file://./data\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Hyperparameter Tuning\n",
    "\n",
    "We can use SageMaker to automated the hyperparameter search process. SageMaker leverages an algorithm called Bayesian Optimization to automate this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a range and set of values we want SageMaker to search across to discover optimal values for the learning rate and batch-size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {'lr': ContinuousParameter(0.001, 0.1),'batch-size': CategoricalParameter([32,64,128,256,512])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the objective metric we seek to optimize. In this case, the objective is to find the hyperparameters that minimizes average loss on our test set. The tuner monitors progress by parsing the training logs. The `metric definitions` describes how to parse the object metric value from the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'average test loss'\n",
    "objective_type = 'Minimize'\n",
    "metric_definitions = [{'Name': 'average test loss',\n",
    "                       'Regex': 'Test set: Average loss: ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a PyTorch estimator. It describes how to execute the training job, the infrastructure and environment. Note that `train_instance_count` is set to two, which will result in distributed training. 'gloo' is selected as the backend for facilitating communication in the distributed training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point='mnist.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.2.0',\n",
    "                    train_instance_count=2,\n",
    "                    train_instance_type='ml.c4.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 6,\n",
    "                        'backend': 'gloo'\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a HyperparameterTuner object, to which we pass:\n",
    "\n",
    "* The PyTorch estimator we created above\n",
    "* Our hyperparameter ranges\n",
    "* Objective metric name and definition\n",
    "* Tuning resource configurations such as Number of training jobs to run in total and how many training jobs can be run in parallel.\n",
    "\n",
    "The code is commented out because the tuner job will require resources and time that is beyond what we have for today's workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuner = HyperparameterTuner(estimator,\n",
    "#                            objective_metric_name,\n",
    "#                            hyperparameter_ranges,\n",
    "#                            metric_definitions,\n",
    "#                            max_jobs=9,\n",
    "#                            max_parallel_jobs=3,\n",
    "#                            objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we'll run a single distributed training job.\n",
    "\n",
    "After we've constructed our `PyTorch` object, we call the fit method to start teh training job. The data is uploaded from the specified S3 location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-22 06:58:36 Starting - Starting the training job...\n",
      "2020-01-22 06:58:38 Starting - Launching requested ML instances......\n",
      "2020-01-22 06:59:38 Starting - Preparing the instances for training......\n",
      "2020-01-22 07:00:56 Downloading - Downloading input data...\n",
      "2020-01-22 07:01:33 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-01-22 07:01:49,531 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-01-22 07:01:49,534 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-22 07:01:49,546 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2020-01-22 07:01:49,974 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2020-01-22 07:01:49,977 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-01-22 07:01:49,988 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2020-01-22 07:01:53,009 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2020-01-22 07:01:53,297 sagemaker-containers INFO     Module mnist does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2020-01-22 07:01:53,297 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2020-01-22 07:01:53,298 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2020-01-22 07:01:53,298 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: mnist\n",
      "  Building wheel for mnist (setup.py): started\n",
      "  Building wheel for mnist (setup.py): finished with status 'done'\n",
      "  Created wheel for mnist: filename=mnist-1.0.0-py2.py3-none-any.whl size=9441 sha256=ed5ab7866f3507806a8f35b642013df6b1061f19b119b68991d7bcc79ccf4aaf\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-790pdt5d/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[35mSuccessfully built mnist\u001b[0m\n",
      "\u001b[35mInstalling collected packages: mnist\u001b[0m\n",
      "\u001b[35mSuccessfully installed mnist-1.0.0\u001b[0m\n",
      "\n",
      "2020-01-22 07:01:48 Training - Training image download completed. Training in progress.\u001b[34m2020-01-22 07:01:52,564 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-01-22 07:01:52,930 sagemaker-containers INFO     Module mnist does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-01-22 07:01:52,930 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-01-22 07:01:52,930 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-01-22 07:01:52,931 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: mnist\n",
      "  Building wheel for mnist (setup.py): started\n",
      "  Building wheel for mnist (setup.py): finished with status 'done'\n",
      "  Created wheel for mnist: filename=mnist-1.0.0-py2.py3-none-any.whl size=9442 sha256=d0bc0a8ca2819a44d19b850bceec1a89ae422946c0ad1f0a61c647acab581bf8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-riclf3nn/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built mnist\u001b[0m\n",
      "\u001b[34mInstalling collected packages: mnist\u001b[0m\n",
      "\u001b[34mSuccessfully installed mnist-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 19.3; however, version 20.0.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-01-22 07:01:55,020 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-22 07:01:55,033 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 6\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-01-22-06-58-35-227\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-407247006381/pytorch-training-2020-01-22-06-58-35-227/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"epochs\":6}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-407247006381/pytorch-training-2020-01-22-06-58-35-227/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":6},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-01-22-06-58-35-227\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-407247006381/pytorch-training-2020-01-22-06-58-35-227/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"6\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=6\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m mnist --backend gloo --epochs 6\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35mWARNING: You are using pip version 19.3; however, version 20.0.1 is available.\u001b[0m\n",
      "\u001b[35mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[35m2020-01-22 07:01:55,204 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-01-22 07:01:55,216 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 6\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-2020-01-22-06-58-35-227\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-407247006381/pytorch-training-2020-01-22-06-58-35-227/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"backend\":\"gloo\",\"epochs\":6}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-west-2-407247006381/pytorch-training-2020-01-22-06-58-35-227/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":6},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2020-01-22-06-58-35-227\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-407247006381/pytorch-training-2020-01-22-06-58-35-227/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"6\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[35mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[35mSM_HP_EPOCHS=6\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python -m mnist --backend gloo --epochs 6\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mDistributed training - True\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 0. Number of gpus: 0\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 30000/60000 (50%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[35mDistributed training - True\u001b[0m\n",
      "\u001b[35mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[35mInitialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 1. Number of gpus: 0\u001b[0m\n",
      "\u001b[35mGet train data loader\u001b[0m\n",
      "\u001b[35mGet test data loader\u001b[0m\n",
      "\u001b[35mProcesses 30000/60000 (50%) of train data\u001b[0m\n",
      "\u001b[35mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [6400/30000 (21%)] Loss: 2.074617\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/30000 (21%)] Loss: 2.076616\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/30000 (43%)] Loss: 1.057926\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [12800/30000 (43%)] Loss: 1.215920\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/30000 (64%)] Loss: 0.937029\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [19200/30000 (64%)] Loss: 0.914584\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/30000 (85%)] Loss: 0.850368\u001b[0m\n",
      "\u001b[35mTrain Epoch: 1 [25600/30000 (85%)] Loss: 0.673603\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.3227, Accuracy: 9100/10000 (91%)\n",
      "\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.3227, Accuracy: 9100/10000 (91%)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/30000 (21%)] Loss: 0.546118\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [6400/30000 (21%)] Loss: 0.496909\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [12800/30000 (43%)] Loss: 0.378099\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/30000 (43%)] Loss: 0.466322\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [19200/30000 (64%)] Loss: 0.544192\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/30000 (64%)] Loss: 0.678142\u001b[0m\n",
      "\u001b[35mTrain Epoch: 2 [25600/30000 (85%)] Loss: 0.493608\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/30000 (85%)] Loss: 0.646309\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1935, Accuracy: 9417/10000 (94%)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.1935, Accuracy: 9417/10000 (94%)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [6400/30000 (21%)] Loss: 0.335214\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [6400/30000 (21%)] Loss: 0.484780\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/30000 (43%)] Loss: 0.339989\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [12800/30000 (43%)] Loss: 0.405359\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [19200/30000 (64%)] Loss: 0.437750\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19200/30000 (64%)] Loss: 0.449370\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/30000 (85%)] Loss: 0.379382\u001b[0m\n",
      "\u001b[35mTrain Epoch: 3 [25600/30000 (85%)] Loss: 0.402836\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.1448, Accuracy: 9550/10000 (96%)\n",
      "\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1448, Accuracy: 9550/10000 (96%)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [6400/30000 (21%)] Loss: 0.161905\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [6400/30000 (21%)] Loss: 0.163557\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [12800/30000 (43%)] Loss: 0.208522\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/30000 (43%)] Loss: 0.346354\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [19200/30000 (64%)] Loss: 0.238450\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19200/30000 (64%)] Loss: 0.683330\u001b[0m\n",
      "\u001b[35mTrain Epoch: 4 [25600/30000 (85%)] Loss: 0.400868\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/30000 (85%)] Loss: 0.257166\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1268, Accuracy: 9617/10000 (96%)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.1268, Accuracy: 9617/10000 (96%)\n",
      "\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [6400/30000 (21%)] Loss: 0.183022\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [6400/30000 (21%)] Loss: 0.232098\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/30000 (43%)] Loss: 0.342575\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [12800/30000 (43%)] Loss: 0.248958\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19200/30000 (64%)] Loss: 0.413875\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [19200/30000 (64%)] Loss: 0.270018\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/30000 (85%)] Loss: 0.191653\u001b[0m\n",
      "\u001b[35mTrain Epoch: 5 [25600/30000 (85%)] Loss: 0.272955\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.1062, Accuracy: 9662/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[35mTest set: Average loss: 0.1062, Accuracy: 9662/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [6400/30000 (21%)] Loss: 0.334178\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [6400/30000 (21%)] Loss: 0.122057\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [12800/30000 (43%)] Loss: 0.212248\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [12800/30000 (43%)] Loss: 0.307393\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [19200/30000 (64%)] Loss: 0.400427\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [19200/30000 (64%)] Loss: 0.359959\u001b[0m\n",
      "\u001b[35mTrain Epoch: 6 [25600/30000 (85%)] Loss: 0.189644\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [25600/30000 (85%)] Loss: 0.177055\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 0.0986, Accuracy: 9696/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34m2020-01-22 07:03:26,895 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-01-22 07:03:38 Uploading - Uploading generated training model\n",
      "2020-01-22 07:03:38 Completed - Training job completed\n",
      "\u001b[35mTest set: Average loss: 0.0986, Accuracy: 9696/10000 (97%)\n",
      "\u001b[0m\n",
      "\u001b[35mSaving the model.\u001b[0m\n",
      "\u001b[35m2020-01-22 07:03:26,948 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 324\n",
      "Billable seconds: 324\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host\n",
    "### Create endpoint\n",
    "After training, we use the `PyTorch` estimator object to build and deploy a `PyTorchPredictor`. This creates a Sagemaker Endpoint -- a hosted prediction service that we can use to perform inference.\n",
    "\n",
    "As mentioned above we have implementation of `model_fn` in the `mnist.py` script that is required. We are going to use default implementations of `input_fn`, `predict_fn`, `output_fn` and `transform_fm` defined in [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers).\n",
    "\n",
    "The arguments to the deploy function allow us to set the number and type of instances that will be used for the Endpoint. These do not need to be the same as the values we used for the training job. For example, you can train a model on a set of GPU-based instances, and then deploy the Endpoint to a fleet of CPU-based instances, but you need to make sure that you return or save your model as a cpu model similar to what we did in `mnist.py`. Here we will deploy the model to a single ```ml.m4.xlarge``` instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could have tested our model serving script using SageMaker Local. The following line of code will deploy the model serving container on this notebook instance so that you can test the endpoint locally.\n",
    "\n",
    "`local_predictor = local_estimator.deploy(initial_instance_count=1, instance_type='local')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "We can now use this predictor to classify hand-written digits. Drawing into the image box loads the pixel data into a `data` variable in this notebook, which we can then pass to the `predictor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open(\"input.html\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "image = np.array([data], dtype=np.float32)\n",
    "response = predictor.predict(image)\n",
    "prediction = response.argmax(axis=1)[0]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p27",
   "language": "python",
   "name": "conda_pytorch_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
